{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"# Summary Statistics\n\nI have shared with you data sets for the books and poems submitted by the students in the course. Add those two data sets to this notebook.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom typing import *\nfrom nltk import tokenize\nimport re\nfrom IPython.display import display, Markdown\nimport os\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Description\n\nOur first attempt to summarize our documents is to calculate and\nvisualize statistical information. One useful technique for visualizing information is formatting it in a *table*. Markdown blocks provide an easy way to create tables:\n\n```\n| Book              |   Author       |\n|-------------------|----------------|\n| Le Morte D'Arthur | Thomas Mallory |\n| Moby Dick         | Herman Melville|\n```\n \n is rendered as:\n \n| Book              |   Author       |\n|-------------------|----------------|\n| Le Morte D'Arthur | Thomas Mallory |\n| Moby Dick         | Herman Melville|\n\nThis format is a convenient way to summarize information. Using the provided function below, you can create and visualize Markdown tables directly from Python lists:\n ","metadata":{}},{"cell_type":"code","source":"def show_markdown_table(headers: List[str], data: List) -> str:\n    s = f\"| {' | '.join(headers)} |\\n| {' | '.join([(max(1, len(header) - 1)) * '-' + ':' for header in headers])} |\\n\"\n    for row in data:\n        s += f\"| {' | '.join([str(item) for item in row])} |\\n\"\n    display(Markdown(s))\n    \nshow_markdown_table(['Book', 'Author'], [[\"Le Morte D'Arthur\", \"Thomas Mallory\"], ['Moby Dick', 'Herman Melville']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accessing Corpus Files\n\nOur books and poems are in a data repository that is included with this notebook. Let's generate a Markdown table to show the book files in the repository:\n","metadata":{}},{"cell_type":"code","source":"\nshow_markdown_table(['Book File Name'], [[file] for file in os.listdir('/kaggle/input/csci-270-books-2022')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here are the poems:","metadata":{}},{"cell_type":"code","source":"show_markdown_table(['Poem File Name'], [[file] for file in os.listdir('/kaggle/input/csci-270-poems-2022')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to open every file in a given directory, and return a dictionary where the keys are the filenames and the values are the contents of the files.","metadata":{}},{"cell_type":"code","source":"def file_dictionary(file_path: str) -> Dict[str,str]:\n    dict = {}\n    for file in os.listdir(file_path):\n        dict[file] = open(file_path + '/' +  file).read()\n    return dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test `file_dictionary()` below. It should display a table of each poem filename along with the number of characters in the file.","metadata":{}},{"cell_type":"code","source":"poems = file_dictionary('/kaggle/input/csci-270-poems-2022')\npoem_lengths = [[filename, len(contents)] for filename, contents in poems.items()]\nshow_markdown_table(['File', '# Chars'], poem_lengths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to return a list of all **letters** in a document. For this and all of the functions we will write, make sure all letters are shifted to lower case.","metadata":{}},{"cell_type":"code","source":"def all_letters_from(text: str) -> List[str]:\n    whitespace = text.replace(' ', '').lower()\n    period = whitespace.replace('.', '')\n    return list(period)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def all_vowels_from(text: str) -> List[str]:\n    vowels = ['a', 'e', 'i', 'o', 'u']\n    whitespace = text.replace(' ', '').lower()\n    period = whitespace.replace('.', '')\n    deez = list(period)\n    for x in period:\n        if (x not in vowels):\n            deez.remove(x)\n    return deez","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter_test = all_letters_from('This is a test.')\nprint(letter_test)\nletter_test == list('thisisatest')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vowel_test = all_vowels_from('This is a test.')\nprint(vowel_test)\nvowel_test == list('iiae')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to return a list of all **tokens** in a document. A **token** is a contiguous sequence of text that consists only of alphanumeric characters.","metadata":{}},{"cell_type":"code","source":"def all_tokens_from(text: str) -> List[str]:\n    token = text.replace('.', '').lower().split()\n    return token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_test = all_tokens_from(\"This is a test.\")\nprint(token_test)\ntoken_test == ['this', 'is', 'a', 'test']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to return a list of all **unique** tokens in a document. You are encouraged to call `all_tokens_from()` as part of your solution.","metadata":{}},{"cell_type":"code","source":"def all_unique_tokens_from(text: str) -> List[str]:\n    unique = []\n    for word in all_tokens_from(text):\n        if word not in unique:\n            unique.append(word)\n    return unique","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_test = all_unique_tokens_from(\"This is a test. This is only a test.\")\nprint(unique_test)\nlen(unique_test) == 5 and all(word in unique_test for word in ['this', 'is', 'a', 'test', 'only'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to return a list of all **sentences** in a document. It is up to you to define a \"sentence\" for this purpose. Strive to come up with a definition that matches our intuitions as closely as possible. Each sentence should be represented as a list of the words it contains.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def all_sentences_from(text: str) -> List[List[str]]:    \n    last_list = [] \n    if \".\" in text:\n        sent_list = tokenize.sent_tokenize(text)\n    if \".\" not in text:\n        sent_list = text.splitlines()\n    for sent in sent_list:\n        new = sent.split(\" \")\n        last_list.append(new)\n    return last_list\n\n    #last_list = [] \n    #wordlist = text.split()\n    #for word in text.split():\n    #    if \"?\" in word:\n    #        deez = word.split('?')\n    #        word = ''.join(deez)\n    #    if \"?\" in word:\n    #        deez = word.split('?')\n    #        word = ''.join(deez)\n    #if \".\" in text:\n    #    sent_list = text.split('.')\n    #if \".\" not in text:\n    #    sent_list = text.splitlines()\n    #for sent in sent_list:\n    #    new_list = sent.split(' ')\n    #    last_list.append(new_list)\n    #return last_list \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def all_sentences_poems(text: str) -> List[List[str]]:\n    return [sentences for sentences in text.splitlines()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a \"sentence\" for our purposes:\n\n**Your answer here**\n\nAdd a code box in which you test `all_sentences_from()`. Your tests should show how your implementation matches your definition.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"sentence_test = all_sentences_from('This is a test. One test. Not many tests. But it is a test? RIGHT!')\nprint(sentence_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## General Statistics\n\nWrite a function that displays a table of the number of characters, letters, sentences, tokens, and unique tokens in each file.","metadata":{}},{"cell_type":"code","source":"def general_statistics(file2text: Dict[str,str]):\n    len_poems = [[filename, len(contents), len(all_letters_from(contents)), len(all_sentences_poems(contents)), len(all_tokens_from(contents)), len(all_unique_tokens_from(contents))] for filename, contents in poems.items()]\n    len_books = [[filename, len(contents), len(all_letters_from(contents)), len(all_sentences_from(contents)), len(all_tokens_from(contents)), len(all_unique_tokens_from(contents))] for filename, contents in books.items()]\n    show_markdown_table(['File', 'Characters', 'Letters', 'Sentences', 'Tokens', 'Unique Tokens'], len_poems)\n    show_markdown_table(['File', 'Characters', 'Letters', 'Sentences', 'Tokens', 'Unique Tokens'], len_books)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"books = file_dictionary('/kaggle/input/csci-270-books-2022')\ngeneral_statistics(poems)\n#general_statistics(books)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Answer the following questions:\n1. How do you feel about your definition of a sentence in light of the above numbers? If it is not adequate, go back and modify it, and regenerate the above tables. If applicable, how did your modifications better match our intuitions about sentences?\n\n\n    I feel as though my definition of a sentence is adequete for our purposes. I could not find a database that lists the number of sentences in any book, so I am not sure how accurate it is. \n\n2. What might the numbers of unique tokens tell us about how these different works compare with each other?\n\n\n    Each book/poem seems to have atleast 10% of its total tokens as unique tokens. If we compared all texts together instead of individually I feel like this number would decrease significantly.\n\n3. What other initial insights can you glean from the above tables?\n\n\n    It seems that the fewer words a text has, the percent of unique tokens it will have increases.","metadata":{}},{"cell_type":"markdown","source":"## Frequency Counts\n\nFrequency counts can yield a lot of useful information about a document. We'll begin by writing several functions to create and visualize frequency counts. Then we will examine the frequency counts of letters, token lengths, and tokens per sentence in your documents.","metadata":{}},{"cell_type":"code","source":"def count(histogram: Dict[Hashable,int], item: Hashable):\n    if item not in histogram:\n        histogram[item] = 1\n    else:\n        histogram[item]+=1\n    return\n    \ndef count_all(items: Iterable[Hashable]) -> Dict[Hashable,int]:\n    return {word:items.count(word) for word in items}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter_example = ['d', 'b', 'a', 'c', 'b', 'a', 'a', 'a']\ncount_test = count_all(letter_example)\nprint(count_test)\ncount_test == {'d': 1, 'b': 2, 'a': 4, 'c': 1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we visualize frequency counts, we would like to have the highest count on the left of the graph, with the remaining counts in descending order. To help do this, we begin by writing a function that takes a dictionary of frequency counts and returns a list of pairs of keys and values, in descending sorted order. \n\nThe `min_count` parameter is the lowest count for inclusion in the output list. This parameter enables us to filter out values that are not well-represented.","metadata":{}},{"cell_type":"code","source":"def find_ranking(histogram: Dict[Hashable,int], min_count=0) -> List[Tuple[Hashable,int]]:\n    return [(key, count) for (count, key) in\n            reversed(sorted([(count, key) for (key, count) in histogram.items() \n                             if count >= min_count]))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranking_test = find_ranking(count_test)\nprint(ranking_test)\nranking_test == [('a', 4), ('b', 2), ('c', 1), ('d', 1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `unzip()` function is a useful utility function. Given a list of tuples, it will return a tuple of lists. This is helpful when transforming the result of `ranking()` into a form we can graph.","metadata":{}},{"cell_type":"code","source":"def unzip(tuple_values):\n    # From https://appdividend.com/2020/10/19/how-to-unzip-list-of-tuples-in-python/#:~:text=%20How%20to%20Unzip%20List%20of%20Tuples%20in,zip...%204%202%3A%20Using%20List%20Comprehension%20More%20\n    return tuple(zip(*tuple_values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a Bar Plot\n\nUsing `matplotlib` (\"`plt`\") to create a bar plot involves the following steps:\n* Use `plt.figure` to create an object representing the plot.\n* Add axes to the figure.\n* Set the `x` and `y` labels of the axes.\n* Plot the bars themselves. \n  * This requires two lists: one for the x values, and one for the y values.\n  * Make sure the x values are strings before plotting.","metadata":{}},{"cell_type":"code","source":"def bar_graph_from(x_label: str, keys2counts: List[Tuple[Hashable,int]]):\n    figure = plt.figure()\n    x = figure.add_axes([0,0,1,1])\n    x.set_xlabel(\"item\")\n    x.set_ylabel(\"count\")\n    x.set_title(x_label)\n    keys, values = unzip(keys2counts)\n    plt.bar(keys, values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frequency_count_graph(x_label: str, items: List[str], min_count=0):\n    histogram = count_all(items)\n    ranked = find_ranking(histogram, min_count)\n    bar_graph_from(x_label, ranked)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequency_count_graph(\"Letters\", letter_example)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add code boxes to generate bar graphs using `frequency_count_graph()` for the following frequency counts, for both your book and your poem:\n* Find the frequency counts for the letters in each document. \n* Find the frequency counts for the lengths of tokens in each document. \n* Find the frequency counts for the numbers of tokens in the sentences in each document. ","metadata":{}},{"cell_type":"code","source":"for filename, contents in poems.items():\n    frequency_count_graph(\"Letters in {}\".format(filename), all_letters_from(contents))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filename, contents in poems.items():\n    dis_list= []\n    for i in all_tokens_from(contents):\n        dis_list.append(len(i))\n    frequency_count_graph(\"Length of tokens in {}\".format(filename), dis_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filename, contents in poems.items():\n    dis_list= []\n    for i in all_sentences_poems(contents):\n        dis_list.append(len(i))\n    frequency_count_graph(\"Average sentence len in {}\".format(filename), dis_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filename, contents in books.items():\n    frequency_count_graph(\"Letters in {}\".format(filename), all_letters_from(contents))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T21:02:34.237143Z","iopub.execute_input":"2022-02-13T21:02:34.237386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filename, contents in books.items():\n    dis_list= []\n    for i in all_tokens_from(contents):\n        dis_list.append(len(i))\n    frequency_count_graph(\"Length of tokens in {}\".format(filename), dis_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T21:02:19.805538Z","iopub.status.idle":"2022-02-13T21:02:19.805938Z","shell.execute_reply.started":"2022-02-13T21:02:19.805772Z","shell.execute_reply":"2022-02-13T21:02:19.805789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filename, contents in books.items():\n    dis_list= []\n    for i in all_sentences_from(contents):\n        dis_list.append(len(i))\n    frequency_count_graph(\"Average sentence len in {}\".format(filename), dis_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T21:02:19.806777Z","iopub.status.idle":"2022-02-13T21:02:19.807161Z","shell.execute_reply.started":"2022-02-13T21:02:19.807011Z","shell.execute_reply":"2022-02-13T21:02:19.807027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Frequency Count Analysis\n\nCompare and contrast the frequency count graphs. What insights about these documents can you obtain from them?\n\n","metadata":{}},{"cell_type":"markdown","source":"## Reading Level\n\nWrite a function to calculate the Flesch-Kincaid Grade Level Formula:\n\n[![](https://readable.io/images/content/4_fkgl.png)](%20https://readable.io/content/the-flesch-reading-ease-and-flesch-kincaid-grade-level/)\n\nUse the **number of vowels** in your document as a substitute for the total number of syllables.","metadata":{}},{"cell_type":"code","source":"def grade_level_formula(text: str) -> float:\n    wrds = len(all_tokens_from(text))\n    sent = len(all_sentences_from(text))\n    vowel = len(all_vowels_from(text))\n    return (0.39 * (wrds / sent ) + 11.8 * ( vowel/ wrds) - 15.59)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assess the reading level of your document using your function. \n\nDoes this match your assumptions? \n\n**Your answer here**\n\nHow does the formula interact with the intuitive basis of your assumptions?\n\n**Your answer here**","metadata":{}},{"cell_type":"code","source":"# Code for grade level of your poem\ngrade_level_formula(poems['The road not taken.txt'])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T21:02:19.810045Z","iopub.status.idle":"2022-02-13T21:02:19.810532Z","shell.execute_reply.started":"2022-02-13T21:02:19.810351Z","shell.execute_reply":"2022-02-13T21:02:19.810372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code for grade level of your book\ngrade_level_formula(books['huck-finn_fixed.txt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Zipf's Law\n\nWrite a function to create a plot of Zipf's Law. In the function:\n* Find the frequncy counts for the tokens in the document. \n* Rank the tokens in descending order based on their counts.\n* Create a log-log plot where the x-axis is the rank of the token, and the\ny axis is the frequency count for that token. \n  * Make sure that both the `x` and `y` values are integers.\n  * Set the axes to log scale after doing everything else to set up the plot.\n* Add to your plot a line graph of Zipf's law, where the y value is the\nfrequency count of the top ranked token divided by the x value.\n\n","metadata":{}},{"cell_type":"code","source":"def token_analysis(text: str):\n    # Your code here, to create a plot of Zipf's Law","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code for Zipf's Law plot of your poem","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code for Zipf's Law plot of your book","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How closely do each of your documents follow Zipf's Law?\n\n**Your answer here**","metadata":{}},{"cell_type":"markdown","source":"Write a function to calculate the fraction of tokens found only once in a document. To do this:\n* Find the frequency counts of the tokens.\n* Find the frequency counts of the frequency counts themselves.\n* Divide the number of frequency counts of `1` by the total number of tokens.\n\nThese tokens are known as *[hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon)* which means \"read only once.\" When translating texts, these tokens are difficult to process because they lack repeated statistical context clues for their meaning.","metadata":{}},{"cell_type":"code","source":"def hapax_legomena_fraction(text: str) -> float:\n    # Your code here","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hltest = hapax_legomena_fraction(\"This is a test. This is only a test. This is really just a test.\")\nprint(hltest)\nhltest == (3/15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write code to display a table of the *hapax legomena* values for each book and poem in our data set.","metadata":{}},{"cell_type":"code","source":"# Write code here to display the table for the poems.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write code here to display the table for the books.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What might you hypothesize or conclude about the works in the corpus from the *hapax legomena* values in the table?\n\n**Your answer here**","metadata":{}},{"cell_type":"markdown","source":"Consolidate all of the text documents into a single string. What is the *hapax legomena* value for the corpus as a whole? ","metadata":{}},{"cell_type":"code","source":"# Write code here to find the hapax legomena value for the corpus as a whole.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What might you infer about the documents from our corpus in light of this summative value?\n\n**Your answer here**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}