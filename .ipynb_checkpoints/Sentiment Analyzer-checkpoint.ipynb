{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "28290319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Kaden's\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40358934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: translate-api in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (4.9.5)\n",
      "Requirement already satisfied: lxml>=4.5.0 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from translate-api) (4.9.1)\n",
      "Requirement already satisfied: requests>=2.25.1 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from translate-api) (2.28.1)\n",
      "Requirement already satisfied: pathos>=0.2.7 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from translate-api) (0.2.9)\n",
      "Requirement already satisfied: PyExecJS>=1.5.1 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from translate-api) (1.5.1)\n",
      "Requirement already satisfied: loguru>=0.4.1 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from translate-api) (0.6.0)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from loguru>=0.4.1->translate-api) (0.4.4)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from loguru>=0.4.1->translate-api) (1.1.0)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from pathos>=0.2.7->translate-api) (0.3.5.1)\n",
      "Requirement already satisfied: pox>=0.3.1 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from pathos>=0.2.7->translate-api) (0.3.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from pathos>=0.2.7->translate-api) (1.7.6.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from pathos>=0.2.7->translate-api) (0.70.13)\n",
      "Requirement already satisfied: six>=1.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from ppft>=1.7.6.5->pathos>=0.2.7->translate-api) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kaden's laptop\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.25.1->translate-api) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install translate-api\n",
    "!pip install translators --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dacd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from nltk import sentiment\n",
    "from IPython.display import display, Markdown\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06a641",
   "metadata": {},
   "source": [
    "# Sentiment_Analyzer\n",
    "\n",
    "#### BOTH OTHER FILES STORED IN THIS REPOSITORY WERE USED AS REFERENCE FOR THIS FILES CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29874af3",
   "metadata": {},
   "source": [
    "We begin by importing the data file and looking at a portion of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406b721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Kaden's Laptop\\Documents\\GitHub\\KadenFranklin.github.io\\sentiment_analyzer\\names1.csv\")\n",
    "print(data.head())\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aef221",
   "metadata": {},
   "source": [
    "The data gathered for this analyis is collected from King Leopold's Congo and the \"Scramble for Africa\" by Michael A. Rutz\n",
    "\n",
    "Specifically from page 163 and the 'Sample of names used in this word'.\n",
    "\n",
    "We can see from the commands above that there are 271 rows of names, with 270 unique names, 188 distinct translations, originating from 91 locations. In addition to this, asside from collective 'meta' names there are 99 names given to colonials by natives.\n",
    "\n",
    "The datatypes are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ad0b7",
   "metadata": {},
   "source": [
    "Local Name - Object\n",
    "\n",
    "Translation - Object\n",
    "\n",
    "Location - Object\t\n",
    "\n",
    "Real Name - Object\n",
    "\n",
    "Now just to be certain we will check the dataset for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7796cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "for x in data.duplicated():\n",
    "    i += 1\n",
    "    \n",
    "    if x == True:\n",
    "        print(x)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f06c72",
   "metadata": {},
   "source": [
    "Success, no duplicates, Now we move on to the interesting part, [sentiment analysis](https://monkeylearn.com/sentiment-analysis/).\n",
    "\n",
    "Now we can split our data into its corresponding columns so that we can work with it more easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b61e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_1 = list(data['Local Names'])\n",
    "transl = list(data[\"Translation\"])\n",
    "location = list(data[\"Location\"])\n",
    "names_2 = list(data['Real Names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6bd84f",
   "metadata": {},
   "source": [
    "This piece of code shows any name with punctuation, it will break when encountering nan or null values and was used for error testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0789440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str_punk = '''!()-[]{};:'\"\\,<>./?@#$%^&*~'''\n",
    "no_p = \"\"\n",
    "\n",
    "for item in names_2:\n",
    "    for el in str_punk:\n",
    "        if el in item:\n",
    "            print(item)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1833d1",
   "metadata": {},
   "source": [
    "The function below defines the [markdown](https://www.markdownguide.org/) table, which the results of our analysis will be stored.\n",
    "\n",
    "As you can see there are also some helper function I've created to help iterate through data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_markdown_table(headers: List[str], data: List) -> str:\n",
    "    s = f\"| {' | '.join(headers)} |\\n| {' | '.join([(max(1, len(header) - 1)) * '-' + ':' for header in headers])} |\\n\"\n",
    "    for row in data:\n",
    "        s += f\"| {' | '.join([str(item) for item in row])} |\\n\"\n",
    "    display(Markdown(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d8a0d",
   "metadata": {},
   "source": [
    "The function below takes in an input of either one name, or a list of names. It will return a resulting markdown table giving the scores of such input names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9063d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_name_analyzer(name):\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()    \n",
    "\n",
    "    return sid.polarity_scores(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58158346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw_name_analyzer(name):\n",
    "    \n",
    "    for x, elem in enumerate(name):\n",
    "\n",
    "        name[x] = ts.google(elem)\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()    \n",
    "\n",
    "    return sid.polarity_scores(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #dis=[]\n",
    "    #sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    #if isinstance(name, list):\n",
    "    #    dis.append([[sid.polarity_scores(x)] for x in name])\n",
    "    #    return dis\n",
    "                \n",
    "    #else:\n",
    "    #    return sid.polarity_scores(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names_1, '\\n')\n",
    "print(sw_name_analyzer(names_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d31171",
   "metadata": {},
   "source": [
    "Having defined such functions we can perform an analysis on our own dataset.\n",
    "\n",
    "The code below is complex, but I will annotate it to the best of my ability.\n",
    " \n",
    " comments are denoted with '#' marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['local name', 'score', 'location', 'translation', 'real name']\n",
    "# This line denotes the first line of output data, our heading\n",
    "\n",
    "rows = [[names_1[num], sw_name_analyzer(names), location[num], \n",
    "         str(transl[num] + \" \" + str(en_name_analyzer(trnl)['compound'])), \n",
    "         str(names_2[num] + \" \" + str(en_name_analyzer(name_2)['compound']))] \n",
    "        for num, (names, name_2, trnl) in enumerate(zip(names_1, names_2, transl))]\n",
    "# This section of code defines the rows that will be generated.\n",
    "# It relies on various other functions and lists define above, but this is the most critical section.\n",
    "\n",
    "# 1st line - local names, local name scores, and location lists\n",
    "# 2nd line - Translation list\n",
    "# 3rd line - Real names / origin list\n",
    "# last line - for #, (variables) in enumerate(three different lists)\n",
    "\n",
    "\n",
    "show_markdown_table(headers, rows)\n",
    "# This function combines the two lines above to compile output\n",
    "\n",
    "# can also do # sw_name_analyzer(names)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac1178",
   "metadata": {},
   "source": [
    "Now that we have performed the necessary evaluation of our names, lets organize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14db94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize by region here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d317b9a",
   "metadata": {},
   "source": [
    "In this file we have used the [Vader](https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664) library to perform Sentiment Analysis on a list of roughly 270 names. Vader allows for the performance of multilingual Sentiment analysis through 3rd party translation APIs. What this means is that inputs from other languages are translated to english in order to calculate a sentiment score. I tried looking for a Natural Language Processing database in Swahili, Kikongo, & Lingala. But resources in computer science are sparse in these langauges.\n",
    "\n",
    "For the reasons above, the words with translated sentiment should be weighted less in the consideration of the word's lasting impact today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
